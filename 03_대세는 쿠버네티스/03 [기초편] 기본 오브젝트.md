---
작성일 : 2020-11-08
강의 주소 : 인프런 - 대세는 쿠버네티스 ^o^ [초급~중급]
---

[TOC]

---

# 기본 오브젝트

## Pod -Container, Label, NodeSchedule

![img](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Pod%20with%20Container,%20Label,%20Node%20Schedule%20for%20Kubernetes.jpg)

<br>

### Container

컨테이너 끼리 Port 같은 것 사용 불가

pot 생성시 **IP 자동할당** (외부에서 접근 불가) - **IP는 휘발성** 있음

![Pod with Container Port for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Pod%20with%20Container%20Port%20for%20Kubernetes.jpg)

- 한 Pod에 여러 컨테이너
- 컨테이너 이름은 p8000 (8000번 포트 open)
- 다른 컨테이너 이름은 p8080 (8080번 포트 open)

#### Pod 생성

```
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
  - name: container1
    image: kubetm/p8000
    ports:
    - containerPort: 8000
  - name: container2
    image: kubetm/p8080
    ports:
    - containerPort: 8080
```

![image-20201202205120898](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202205120898.png)

Dashboard에서 생성 후, 위와 같이 정보를 볼 수 있다.

<br>

![image-20201202205325392](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202205325392.png)

master에서 해당 pod에 대해 8000, 8080 포트로 접근 가능

<br>

![image-20201202205500114](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202205500114.png)

exec 버튼을 클릭해, pod의 container에 직접 접근 가능



![image-20201202205649689](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202205649689.png)

Container 1에서 같은 pod의 Container 2에 접근 가능

<br>

>   ![image-20201202205957332](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202205957332.png)
>
>   같은 pod에서 동일한 port의 Container를 만드려고 하면 위와 같이 실패해서 재생성 계속 시도.

<br>

#### Replication Controller생성

>   Controller는 pod를 만들어주고 pod가 죽었을 때 다시 생성해주는 관리적 역할

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: replication-1
spec:
  replicas: 1
  selector:
    app: rc
  template:
    metadata:
      name: pod-1
      labels:
        app: rc
    spec:
      containers:
      - name: container
        image: kubetm/init
```

![image-20201202210349012](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202210349012.png)

`replication-1-8srxn`을 삭제하면 바로 `replication-1-z4l6m`이 자동으로 생성됨.

<br>

### Label

![Pod with Label Selector for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Pod%20with%20Label%20Selector%20for%20Kubernetes.jpg)

**Label** : 목적에 따라 오브젝트들을 분류하고 분류된 오브젝트만 따로 관리

key :Value 로 나뉨

개발에서 돌아가는 web, db, server

상용에서 돌아가는 web, db, server로 나뉨

상용에 사용되는 라벨을 서비스에 원하는 pod를 붙여 사용하도록 만듬

#### Pod 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  labels:
    type: web
    lo: dev
spec:
  containers:
  - name: container
    image: kubetm/init
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-2
  labels:
    type: db
    lo: dev
spec:
  containers:
  - name: container
    image: kubetm/init
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-3
  labels:
    type: server
    lo: dev
spec:
  containers:
  - name: container
    image: kubetm/init
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-4
  labels:
    type: web
    lo: production
spec:
  containers:
  - name: container
    image: kubetm/init
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-5
  labels:
    type: db
    lo: production
spec:
  containers:
  - name: container
    image: kubetm/init
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-6
  labels:
    type: server
    lo: production
spec:
  containers:
  - name: container
    image: kubetm/init
```

<br>

#### Service 생성

```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-for-web
spec:
  selector:
    type: web
  ports:
  - port: 8080
```

![image-20201202211202340](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202211202340.png)

`Type: web`인 pod들이 연결됨을 확인.

<br>

```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-for-prod
spec:
  selector:
    lo: production
  ports:
  - port: 8080
```

![image-20201202211332579](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202211332579.png)`lo: production`인 pod들이 연결됨을 확인.

> 나중에 서비스를 구동할 때 Selector에 있는 라벨에 해당하는 Pod들만 연결이 됨

<br>

### Node Schedule

Pod는 여러 노드들 중 하나에 올라가야하는데

직접 선택하는 방법과 자동으로 선택되는 것이 있음

![Pod with Node Schedule for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Pod%20with%20Node%20Schedule%20for%20Kubernetes.jpg)

#### 직접선택  Pod 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-3
spec:
  nodeSelector:
    kubernetes.io/hostname: k8s-node1
  containers:
  - name: container
    image: kubetm/init
```

![image-20201202211742313](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202211742313.png)

k8s-node1의 레이블에서 node생성할 때 nodeSelector와 동일한 게 있다.

<br>

![image-20201202212013768](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202212013768.png)

k8s-node1 에 생성됨.

<br>

#### 스케쥴러 판단 Pod생성

<img src="img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202212333280.png" alt="image-20201202212333280" style="zoom:50%;" /><img src="img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202212351681.png" alt="image-20201202212351681" style="zoom:50%;" />

Node1은 메모리가 부족하고 Node2는 여유롭다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-5
spec:
  containers:
  - name: container
    image: kubetm/init
    resources:
      requests:
        memory: 2Gi
      limits:
        memory: 3Gi
```

![image-20201202212957902](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202212957902.png)

- Memory 초과시 pod 종료시킴
- Cpu 초과시 request로 낮춤, Over시에도 종료되지 않음

>   CPU가 limits 수치까지 올라갔다고 해서 무조건 Request 수치까지 core를 낮추는 것이 아니고, Node의 전체 부하 상태가 OverCommit된 상태일때 동작합니다. Node 위의 Pod들이 Node의 자원을 모두 사용하고도 그 이상을 자원을 요구하게 되었을 때 Limit 수치까지 CPU를 사용하고 있는 Pod들에 한해서 그 수치를 Reqeust까지 떨어뜨리게 되며 Memory의 경우에도 그러한 상태일때 Limit까지 올라간 Pod들에 한해 재기동 시키게 됩니다. 

<br>

<br>

## Service - ClusterIP, NodePort, LoadBalancer

![img](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Service%20with%20ClusterIP,%20NodePort,%20LoadBalancer.jpg)

서비스는 기본적으로 자신의 클러스터 iP를 가지고 있고 pod를 연결시켜놓으면 서비스로부터 pod 접근가능.

서비스를 통해서 접근하는 이유 : pod는 언제든지 장애로 죽을 수 있고 재생성 되는데 IP가 변할 수 있기 때문

서비스는 사용자가 직접 지우지 않는 이상, 지워지지 않아 항상 접근 할 수 있음

서비스의 종류가 다양함. (pod 접근 방식 바뀜)

<br>

### ClusterIP

![Service with ClusterIP for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Service%20with%20ClusterIP%20for%20Kubernetes.jpg)

- 외부에서 접근 할 수 없지만, 클러스터 내에서는 접근 가능
- 한 서비스에서 여러 pod 연결 가능
- 서비스가 트래픽 분산해서 전달
- type 생략 시 기본값으로 설정됨
- targetPort는 사용될 서비스의 pod port를 의미함

인가된 사용자(운영자) - 외부에서 접근 불가, 내부에서만 사용할 수 있음

작업 : 쿠버네티스 내부 대시보드 관리

작업 : pod의 서비스 상태 디버깅

#### Pod 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  labels:
     app: pod
spec:
  nodeSelector:
    kubernetes.io/hostname: k8s-node1
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
```

#### Service 생성

```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-1
spec:
  selector:
    app: pod
  ports:
  - port: 9000
    targetPort: 8080
```

>   `type: ClusterIP` 지정 하지않아도 default 값이라 ClusterIP로 됨.

![image-20201202215639248](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202215639248.png)

master에서 서비스IP, port로 curl 날리면 pod-1으로 접속한게 확인 됨

Cluster 밖에서 접근은 불가

pod를 삭제하고 동일하게 재 생성해도 service연결됨. pod IP는 변하지만(`1-1`) Service IP는 안변함

<br>

### NodePort

![Service with NodePort for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Service%20with%20NodePort%20for%20Kubernetes.jpg)

- ClusterIP의 성격 + a
- 모든 노드에 똑같은 port가 할당되어 노드의 IP, Port로 접근하면 서비스로 연결됨
- pod가 있는 노드 뿐아니라 모든 노드에 port할당됨
- 1번 노드로 접근해도 서비스가 2번 pod로 전달해주기도 함
- externalTrafficPolicy 설정이 되어있다면 노드로 직접 접근시 접근한 노드로 서비스 전달

내부망 연결

데모나 임시 연결용 - 내부적인 개발을 하다가 잠깐 보일때

<br>

#### Service 생성

```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-2
spec:
  selector:
    app: pod
  ports:
  - port: 9000
    targetPort: 8080
    nodePort: 30000
  type: NodePort
#  externalTrafficPolicy: Local
```

![image-20201202223623792](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202223623792.png)

Cluster외부에서 node로 접근할때는 30000 port(nodePort)를 이용해서 접근

Cluster내부에서 Service로 접근할때는 9000 port를 이용해서 접근

**알아서 Load Balancing 해줌!**

>   externalTrafficPolicy: Local 를 추가하며 Service를 만들면 node1에서는 pod1만 접근, node2에서는 pod2만 접근

<br>

### Load Balancer

![Service with LoadBalancer for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Service%20with%20LoadBalancer%20for%20Kubernetes.jpg)

- NodePort의 성격 + a
- Load Balancer가 생성되지만 외부 플러그인이 설치되어있어야지 외부 IP 지원

외부 시스템 노출용 (서비스 배포)

<br>

#### Service 생성

```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-3
spec:
  selector:
    app: pod
  ports:
  - port: 9000
    targetPort: 8080
  type: LoadBalancer
```

![image-20201202223955612](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202223955612.png)

LoadBalancer는 외부IP를 매핑해주는 플러그인이 없으면 위와 같이 svc-3이 pending 상태 지속.

master노드에서 `kubectl get service svc-3`명령어로 확인 시 

![image-20201202224152922](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202224152922.png)

`EXTERNAL-IP`가 pending임을 확인 가능

<br>

<br>

## Volume - emptyDir, hostPath, PV/PVC

![img](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Volume%20with%20emptyDir,%20hostPath,%20PV,%20PVC,%20for%20Kubernetes.jpg)

### emptyDir

>   일종의 **pod공유디렉토리**

![Volume with emptyDir for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Volume%20with%20emptyDir%20for%20Kubernetes.jpg)

- 컨테이너들끼리 데이터 공유
- 최초 볼륨 생성될 떄 볼륨이 비어있어서 empty
- 두 컨테이너가 볼륨을 마운트 해놓으면 컨테이너가 자연스럽게 공유 할 수 있음
- Pod 생성시 만들어지고 **삭제시 없어져서 일시적인 내용을 담을 때** 사용할 수 있도록.
- 컨테이너1은 /mount1, 컨테이너2는 /mount2이지만 실질적인 mount는 똑같음

#### Pod 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-volume-1
spec:
  containers:
  - name: container1
    image: kubetm/init
    volumeMounts:
    - name: empty-dir
      mountPath: /mount1
  - name: container2
    image: kubetm/init
    volumeMounts:
    - name: empty-dir
      mountPath: /mount2
  volumes:
  - name : empty-dir
    emptyDir: {}
```

두 컨테이너가 자신이 정한 path의 volumes에 접근.

>   Container1에서는 `/mount1`이라하고  Container2에서는 `/mount2`라고 하지만, 모두 동일한 volumes을 바라보고 있다. **이름만 다르다!**

![image-20201202225250517](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202225250517.png)

Container 1에서 mount1이 mount됨을 확인하였고 거기에 `file.txt`를 생성하였다.

<br>

![image-20201202225406797](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202225406797.png)

Container 2에서 mount2가 mount됨을 확인하였고 Container1에서 생성한 `file.txt` 가 있음!

>   **Volume이 Pod내에 존재하기 때문에 Pod가 삭제되면 Volume도 삭제된다**
>
>   Volume이 사라져도 되고, pod내 Container끼리 통신하기 위한 파일들만 emptyDir을 사용

<br>

### hostPath

>   일종의 **Node내에서 공유디렉토리** (pod가 다른 노드에 생성되면 접근불가)

![Volume with hostPath for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Volume%20with%20hostPath%20for%20Kubernetes.jpg)

- pod가 죽어도 data가 살아지지 않음
- pod2가 죽어서 재생성되었는데 다른 노드에서 생성된다면, node-v1의 폴륨을 사용할 수 없음
- 대신 node추가시마다 Mount걸어주면 할 수 있음(쿠버네티스가 해주는 기능이 아니라 귀찮음)
- pod자신이 할당되어있는 host의 파일을 읽고 쓸때 (설정 파일 등)
- host-path라는 이름의 볼륨과 속성.
- 사전에 해당 Node에 경로가 있어야함. (`type: DirectoryOrCreate`는 없으면 생성됨)
- **하드에 데이터 저장용 X**
- **노드에 있는 데이터를 pod에서 쓰기위함 O**

#### Pod 생성

이름만 다른 pod-volume-2, 3만들기

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-volume-3
spec:
  nodeSelector:
    kubernetes.io/hostname: k8s-node1
  containers:
  - name: container
    image: kubetm/init
    volumeMounts:
    - name: host-path
      mountPath: /mount1
  volumes:
  - name : host-path
    hostPath:
      path: /node-v
      type: DirectoryOrCreate
```

![image-20201202230117215](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202230117215.png)

pod-volume-2에서 생성한 `file.txt`가 pod-volume-3에도 있음.

<br>

>   ![image-20201202230337622](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202230337622.png)
>
>   k8s-node1에서 `node-v` 디렉토리가 있는 것을 확인할 수 있음.
>
>   pod를 삭제하고 재생성해도 k8s-node1에서 `node-v` 디렉토리가 사라지는 게 아니라 남아있음.

<br>

### PV/PVC

>   Volume을 따로 만들어 놓기 때문에 **node에 상관없이, pod에 상관없이** 접근 가능!!

![Volume with PersistentVolume PersistentVolumeClaim for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Volume%20with%20PersistentVolume%20PersistentVolumeClaim%20for%20Kubernetes.jpg)

- pod의 영속성 개념 제공
- aws, git 등 다른 곳의 volume을 사용하는 경우, persistent volume(PV)을 통해 PVC를 두어 pod를 연결
- User, Admin으로 나뉨
- User는 pod개발자
- Admin은 쿠버네티스 관리자
- PVC를 이용해 관리자에게 요청하는 느낌
- claimName에 PVC네임을 입력함
- PV정의 생성 - PVC 생성 - PV 연결 - pod생성시 PVC 마운트

<br>

#### PersistentVolume (PV) 생성

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-01
spec:
  capacity:
    storage: 1G
  accessModes:
  - ReadWriteOnce
  local:
    path: /node-v
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - {key: kubernetes.io/hostname, operator: In, values: [k8s-node1]}
```

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-02
spec:
  capacity:
    storage: 1G
  accessModes:
  - ReadOnlyMany
  local:
    path: /node-v
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - {key: kubernetes.io/hostname, operator: In, values: [k8s-node1]}
```

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-03
spec:
  capacity:
    storage: 2G
  accessModes:
  - ReadWriteOnce
  local:
    path: /node-v
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - {key: kubernetes.io/hostname, operator: In, values: [k8s-node1]}
```

<br>

#### PersistentVolumeClaim (PVC) 생성

`accessModes`가 동일하고 `storage`가 PV보다 작아야지만 연결됨. (아닌 경우 Pending)

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-01
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1G
  storageClassName: ""
```

![image-20201202231444534](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202231444534.png)

accessModes가 `ReadWriteOnce`이고 `storage: 1G`이므로 **pv-01**과 연결됨.

연결되면 해당 pv는 다른 pvc에서 연결할 수 없음

<br>

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-02
spec:
  accessModes:
  - ReadOnlyMany
  resources:
    requests:
      storage: 1G
  storageClassName: ""
```

![image-20201202231626199](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202231626199.png)

accessModes가 `ReadOnlyMany`이고 `storage: 1G`이므로 **pv-02**과 연결됨.



```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-03
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1G
  storageClassName: ""
```

![image-20201202231940296](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202231940296.png)

accessModes가 `ReadWriteOnce`이고 `storage: 1G`이지만 pv-03의 storage보다 작아서 **pv-03**과 연결됨.

<br>

#### Pod 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-pv-pvc
spec:
  containers:
  - name: container
    image: kubetm/init
    volumeMounts:
    - name: pvc-pv
      mountPath: /mount3
  volumes:
  - name : pvc-pv
    persistentVolumeClaim:
      claimName: pvc-01
```

![image-20201202232221684](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201202232221684.png)

아까만든 k8s-node1의 `node-v/` 디렉토리를 **pv-01**로 만들었기 때문에 파일이 존재함.

<br>

<br>

## ConfigMap, Secret - Env, Mount

![ConfigMap, Secret with Literal, File on Env, Mount for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/ConfigMap,%20Secret%20with%20Literal,%20File%20on%20Env,%20Mount%20for%20Kubernetes.jpg)

<br>

### ConfigMap, Secret

![image-20201108233358521](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201108233358521.png)

- 개발환경과 상용환경에서 보안 설정 등 때문에 똑같은 이미지를 2개 가지는 건 비효율적
- 환경에 따라 옵션 변경할 수 있게 하는 것이 ConfigMap과 Secret
- 똑같은 Container를 이용해 두 환경에 사용가능

<br>

### Env (Literal 상수)

![ConfigMap, Secret with Literal for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/ConfigMap,%20Secret%20with%20Literal%20for%20Kubernetes.jpg)

- 환경변수를 이용해서 하는 방법
- PW, key는 Secret에 저장됨. base64로 변환해서 저장해야함(메모리에 저장되어 보안이 좋음 1Mbyte 제한)

#### ConfigMap 생성

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cm-dev
data:
  SSH: 'false'
  User: dev
```

key, value 값은 string으로 해야함

#### Secret 생성

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: sec-dev
data:
  Key: MTIzNA==
```

base64인고딩을 안한 key를 생성하면 에러발생

#### Pod 생성

``` yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
  - name: container
    image: kubetm/init
    envFrom:
    - configMapRef:
        name: cm-dev
    - secretRef:
        name: sec-dev
```

Container에 접속에서 `ENV`명령어를 통해 확인해보면 환경변수가 들어감을 확인가능

![image-20201203223208272](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201203223208272.png)

<br>

### Env (File)

![ConfigMap, Secret with File for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/ConfigMap,%20Secret%20with%20File%20for%20Kubernetes.jpg)

- 파일 이름이 Key이므로
- 파일 만드는 건 dashboard에서 지원하지 않기 때문에 bash이용해서 만들어야함
- file.txt 내용이 base64로 인코딩됨
- 환경변수의 이름은 **file** 
- configMap(파일)의 내용이 변해도 pod Content 변경되지 않음(죽어서 재생성되면 바뀜)

>   대시보드의 컨피그맵과 시크릿에서 확인가능

#### ConfigMap 생성

```sh
echo "Content" >> file-c.txt
kubectl create configmap cm-file --from-file=./file-c.txt
```

#### Secret 생성

```bash
echo "Content" >> file-s.txt
kubectl create secret generic sec-file --from-file=./file-s.txt
```

`kubectl create secret generic`명령어에 의해 알아서 base64 인코딩됨

#### Pod 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-file
spec:
  containers:
  - name: container
    image: kubetm/init
    env:
    - name: file-c
      valueFrom:
        configMapKeyRef:
          name: cm-file
          key: file-c.txt
    - name: file-s
      valueFrom:
        secretKeyRef:
          name: sec-file
          key: file-s.txt
```

![image-20201203223533695](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201203223533695.png) `ENV`치면 이런식으로 나옴

<br>

### Volume Mount

![ConfigMap, Secret with Mount for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/ConfigMap,%20Secret%20with%20Mount%20for%20Kubernetes.jpg)

- file을 configMap에 넣는 것 까지는 똑같음.
- pod만들때 mount path정함 
- configMap의 내용이 변하면 pod Content 변경

#### Pod 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-mount
spec:
  containers:
  - name: container
    image: kubetm/init
    volumeMounts:
    - name: file-volume
      mountPath: /mount
  volumes:
  - name: file-volume
    configMap:
      name: cm-file
```

<br>

<br>

## Namespace, ResourceQuota, LimitRange

![Namespace, ResourceQuota, LimitRange for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/Namespace,%20ResourceQuota,%20LimitRange%20for%20Kubernetes.jpg)

쿠버네티스 클러스터 전체 자원 (메모리, CPU 등)

네임스페이스들이 다양하고 pod도 많은데 특정 네임스페이스 pod가 완전히 사용하면 다른 pod이 사용불가하기 때문에

pod의 자원이 Quota 이상 자원을 사용할 수 없게 막아주는 기능.

Limit Range를 둬서 들어올 pod의 크기를 조정할 수 있음

<br>

### Namespace

![How to use Namespace for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/How%20to%20use%20Namespace%20for%20Kubernetes.jpg)

- 한 네임스페이스에서 같은 이름의 pod를 지정할 수 없음
- 타 네임스페이스의 자원과 분리되서 관리됨.
- 네임스페이스를 지우면 그 내부 자원들도 모두 제거됨.
- 타 네임스페이스에서 pod가 접근시 접근은 가능
- 네임스페이스는 이름만 지정
- selector와 label이 같아도 연결되지 않음
- 오브젝트끼리의 연결은 같은 네임스페이스 안에서만 됨

#### Namespace 생성

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: nm-1
```

#### Pod 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
  namespace: nm-1
  labels:
    app: pod
spec:
  containers:
  - name: container
    image: kubetm/app
    ports:
    - containerPort: 8080
```

`namespace: nm-1`가 없어도 dashboard에서 생성할 때 네임스페이스가 nm-1이면 nm-1에 생성됨.

>   같은 네임스페이스에 동일한 pod를 만들면 이름이 겹쳐서 만들 수 없음.
>
>   하지만 다른 네임스페이스라면 이름이 같은 pod만들어도 상관없음.

#### Service 생성

```yaml
apiVersion: v1
kind: Service
metadata:
  name: svc-1
  namespace: nm-1
spec:
  selector:
    app: pod
  ports:
  - port: 9000
    targetPort: 8080
```

같은 네임스페이스 안의 pod만 연결됨.

>   ![image-20201203224505793](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201203224505793.png)
>
>   하지만 다른 네임스페이스라도 pod의 통신을 막지않음!
>
>   NodePort인 때에 다른 네임스페이스라도 동일한 포트를 사용해서 서비스를 만들 수 없음

<br>

### ResourceQuota

![How to use ResourceQuota for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/How%20to%20use%20ResourceQuota%20for%20Kubernetes.jpg)

- 네임스페이스의 자원한계를 설정 (메모리, cpu, storage)
- request와 limits으로 나눔.
- pod생성시 무조건 스펙을 지정해야함.
- 새로운 pod가 스펙이 없거나 넘어서면 들어올 수 없음.
- ResourceQuota 생성 후 -> Pod를 생성해야함.
- Pod 생성 후 -> ResouceQuota 생성하면 원하는데로 제한이 안됨

#### Namespace 생성

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: nm-3
```

#### ResourceQuota 생성

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: rq-1
  namespace: nm-3
spec:
  hard:
    requests.memory: 1Gi
    limits.memory: 1Gi
#    pods: 2  # pod의 개수 제한
```

ResourceQuota는 콘솔에서만 확인

![image-20201203225137460](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201203225137460.png)

`kubectl describe resourcequotas --namespace=[NAMESPACE]`

#### Pod 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-2
spec:
  containers:
  - name: container
    image: kubetm/app
# Namespace가 ResouceQuota에 제한되어있으므로 resources 없이는 pod 생성 불가
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-3
spec:
  containers:
  - name: container
    image: kubetm/app
    resources:
      requests:
        memory: 0.5Gi
      limits:
        memory: 0.5Gi
```

#### 초과된 메모리 요청시

![image-20201203225426258](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/image-20201203225426258.png)

1Gi가 최대양이고 현재 512Mi를 사용하고 있는데 800Mi 요청이 들어와 생성할 수 없다.

<br>

### LimitRange

![How to use LimitRange1 for Kubernetes](img/03%20%5B%EA%B8%B0%EC%B4%88%ED%8E%B8%5D%20%EA%B8%B0%EB%B3%B8%20%EC%98%A4%EB%B8%8C%EC%A0%9D%ED%8A%B8/How%20to%20use%20LimitRange1%20for%20Kubernetes.jpg)

- 각각의 pod가 들어올 때 해당 네임스페이스에 들어올 수 있는지 체크
- pod에서 생성되는 메모리 min, max와 maxLimitRequestRatio 리밋과 request 배율 값
- defaultRequest, default는 설정되지 않은 pod에 들어감.
- 각각의 pod마다 지원되는 옵션 다름.

#### Namespace 생성

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: nm-5
```

#### LimitRange 생성

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: lr-1
spec:
  limits:
  - type: Container
    min:
      memory: 0.1Gi
    max:
      memory: 0.4Gi
    maxLimitRequestRatio:
      memory: 3
    defaultRequest:
      memory: 0.1Gi
    default:
      memory: 0.2Gi
```

LimitRange도 콘솔에서만 확인

`kubectl describe limitranges --namespace=[NAMESPACE]`

#### Pod 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
  - name: container
    image: kubetm/app
    resources:
      requests:
        memory: 0.1Gi
      limits:
        memory: 0.5Gi
# memory 최대는 0.4Gi인데 0.5Gi를 요청했고, ratio가 3보다 큰 5라 이 pod생성 불가
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-2
spec:
  containers:
  - name: container
    image: kubetm/app
# resource지정 안하면 default값인 requests.memory: 0.1Gi, limits.memory: 0.2Gi로 생성
```

#### 주의

두 개의 LimitRange에서 동일 항목에 다른 값을 제한 걸면 정상적으로 pod가 생성되지 않을 수도 있다.